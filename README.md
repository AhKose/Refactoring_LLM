# Replication Package: Empirical Analysis of LLM-Driven Refactoring for Microservices

This repository serves as the replication package for the study titled **"An Empirical Analysis of LLM-Driven Refactoring for Microservice Systems."**

It contains the experimental datasets, the 3-phase prompting protocol, the raw code artifacts generated by three state-of-the-art Large Language Models (LLMs), and the human evaluation labels used to assess the reliability of AI-supported refactoring in distributed systems.

## üìå Abstract

Large Language Models (LLMs) are increasingly used in software engineering, raising the question of whether they can reliably support refactoring in microservice‚Äëbased systems, where architectural constraints and deployment requirements make refactoring particularly complex. This study empirically examines LLM‚Äëdriven refactoring using a structured, three‚Äëphase prompting protocol applied to four microservices previously labeled as low‚Äëmaintainability by experienced developers. 

Three models‚Äî**GPT‚Äë5.2 Extended Thinking**, **Claude Opus 4.5**, and **Gemini 3 Pro**‚Äîwere evaluated across analysis, refactoring, and self‚Äëaudit phases, followed by deployment, execution, and metric‚Äëbased validation. Of twelve total refactorings, eleven successfully passed build and run verification. Overall, the findings indicate that LLM‚Äëdriven, developer‚Äëassisted refactoring can produce measurable and functionally safe improvements in microservice‚Äëbased systems.

## üìÇ Repository Structure

The repository is organized to reflect the **Input $\rightarrow$ Process $\rightarrow$ Output $\rightarrow$ Evaluation** workflow:

### 1. üìÑ `Prompt.md` (Experimental Protocol)
Documents the **3-Stage Chain-of-Thought Prompting Strategy** utilized in the study.
* **Turn 1:** Context Loading & Architectural Analysis.
* **Turn 2:** Refactoring Implementation (Code Generation).
* **Turn 3:** Self-Audit & Justification.

### 2. üì• `LLM_Input/` (Context & Targets)
Contains the source artifacts provided to the LLMs.
* **Target Files:** The specific source code files identified for refactoring (write-access).
* **Context Files:** Read-only dependencies (e.g., `pom.xml`, DTOs) provided to ensure architectural consistency.

### 3. üì§ `LLM_Outputs/` (Generated Artifacts)
Contains the raw **refactored source code** generated by the models in *Turn 2*.
* **Content:** This directory includes **only the specific files that were modified** by the models.
* **Note:** These files represent the raw LLM output before the manual integration phase.

### 4. üè∑Ô∏è `Refactoring_LLM/Labels.md` (Human Evaluation)
Contains the qualitative assessment data.
* **Methodology:** A blind review process by three senior developers.
* **Metrics:** Individual ratings and **Consensus Scores** (Low/Medium/High) regarding correctness, readability, and API stability.

---

## üî¨ Methodology & Workflow

The study followed a semi-automated, human-in-the-loop workflow:

1.  **Context Extraction:** Relevant code contexts were extracted from the target microservices.
2.  **LLM Inference:** The 3-phase prompt was executed on GPT-5.2, Claude Opus 4.5, and Gemini 3 Pro.
3.  **Manual Integration:** The code snippets generated in *Turn 2* (found in `LLM_Outputs`) were manually integrated into the original microservice source trees, replacing the old logic.
4.  **Verification:** The refactored microservices underwent build and runtime testing.
5.  **Metric Extraction:** Static code analysis metrics were collected to measure improvements in complexity and maintainability.

---

## üìä Key Findings

The study evaluated **12 service‚Äìmodel refactoring outcomes** (4 Services √ó 3 Models).

* **Reliability:** **11 out of 12** refactorings successfully passed the build and runtime verification pipeline.
* **Metric Improvements:**
    * **Cognitive Complexity (Max):** Reduced by approx. **50%**.
    * **Cyclomatic Complexity (Max):** Reduced by approx. **30%**.
    * **Code Duplication:** Decreased by **7.5%**.
    * **Code Size:** LOC increased by ~6.4% and Number of Methods (NoM) by ~30%, consistent with the enforced decomposition strategy (extracting private helper methods).
* **Model Comparison:**
    * **Claude Opus 4.5:** Exhibited the most stable behavior and strongest reductions in complexity.
    * **Gemini 3 Pro:** Achieved competitive improvements with the smallest increase in code size (LOC).
    * **GPT-5.2 Extended Thinking:** Produced the most aggressive changes but resulted in the only build-time failure.

## üéØ Target Systems

The experiment was conducted on microservices from the following open-source benchmarks:

* **[TeaStore](https://github.com/DescartesResearch/TeaStore):** A reference microservices application for benchmarking.
* **[Microservices-observability](https://github.com/aelkz/microservices-observability):** A project demonstrating observability patterns.

---

## ‚úâÔ∏è Contact & Citation

If you utilize this dataset or replication package in your research, please refer to the associated workshop paper. For inquiries regarding dataset integrity, please open an Issue in this repository.
