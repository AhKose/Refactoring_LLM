# An Empirical Analysis of LLM-Driven Refactoring for Microservice Systems

This repository contains the replication package, datasets, and evaluation metrics for the workshop study regarding the effectiveness of **Large Language Models (LLMs)** in automating architectural refactoring for microservices.

## ðŸ“Œ Abstract

The objective of this study is to evaluate the capability of LLMs to generate valid, context-aware refactoring suggestions for distributed systems. We employed a human-in-the-loop evaluation methodology where source code contexts from open-source benchmarks were fed into LLMs, and the resulting outputs were assessed by three independent senior developers to reach a consensus on quality and applicability.

---

## ðŸ“‚ Repository Structure

This repository is organized into three primary directories representing the input-process-output flow of the experiment:

### 1. `LLM_Input/` (Experimental Dataset)
This directory contains the prompt engineering context and source code artifacts used to query the LLMs.
* **Contents:** Contextualized code snippets, architectural descriptions, and specific file dependencies for each microservice.
* **Purpose:** To provide the necessary constraints and scope for the LLM to understand the specific service domain before generating refactoring suggestions.

### 2. `LLM_Outputs/` (Generated Artifacts)
This directory houses the raw responses generated by the Large Language Models.
* **Contents:** Unfiltered refactoring proposals, code modifications, and architectural reasoning provided by the models.
* **Format:** The outputs are stored in text/markdown format, preserving the original generation structure.

### 3. `Refactoring_LLM/` (Human Evaluation & Ground Truth)
This directory contains the qualitative assessment of the generated outputs.
* **File:** `Labels.md`
* **Methodology:** The LLM outputs were subjected to a **blind review process by three distinct developers**.
* **Metrics:** Each suggestion was rated on a scale (Low, Medium, High) based on feasibility and correctness. The file includes the individual ratings and the final **Consensus Score** derived from the inter-rater agreement.

---

## ðŸ”¬ Methodology

The study followed a three-step experimental protocol:

1.  **Context Extraction:** Relevant codebases were parsed to extract service-specific contexts (See `LLM_Input`).
2.  **Inference:** The contexts were processed by the LLM to generate refactoring strategies (See `LLM_Outputs`).
3.  **Triangulated Assessment:** The outputs were evaluated by three domain experts. The consensus data is visualized in `Refactoring_LLM/Labels.md`.

## ðŸŽ¯ Case Studies

The following open-source microservice
