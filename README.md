# An Empirical Analysis of LLM-Driven Refactoring for Microservice Systems

This repository serves as the replication package for the workshop study evaluating the effectiveness of **Large Language Models (LLMs)** in developerâ€‘assisted refactoring. It contains the developer labeling, experimental inputs, prompting strategies, and the resulting code artifacts used to assess the quality of LLM-generated refactorings.

## ğŸ“Œ Overview

The objective of this study is to determine whether LLMs can successfully identify architectural smells and generate valid refactoring code for distributed systems. The study employs a **human-in-the-loop** evaluation methodology, where code generated by LLMs is reviewed by domain experts.

## ğŸ“‚ Repository Structure

The repository is organized to reflect the experimental workflow: **Input Context $\rightarrow$ Prompting Strategy $\rightarrow$ Code Generation $\rightarrow$ Evaluation**.

### 1. ğŸ“„ `Prompt.md` (Experimental Protocol)
This file documents the **3-Stage Prompting Strategy** utilized in the study. It details the systematic instructions provided to the LLMs to guide them from architectural analysis to code generation.
* **Stage 1:** Role definition and architectural comprehension.
* **Stage 2:** Smell detection and refactoring planning.
* **Stage 3:** Implementation and code generation.

### 2. ğŸ“¥ `LLM_Input/` (Context & Targets)
This directory contains the source artifacts fed into the LLMs during the prompting phase. The inputs are categorized into two types to simulate a realistic development environment:
* **Target Files:** The specific source code files identified for refactoring (write-access).
* **Context Files (Read-Only):** Supporting files and dependencies provided to the LLM to ensure it understands the broader system architecture and logic, without requesting changes to them.

### 3. ğŸ“¤ `LLM_Outputs/` (Generated Artifacts)
This directory contains the **refactored source code** generated by the LLMs.
* **Content:** To facilitate clear comparison and diff analysis, this directory includes **only the specific files that were modified** by the models.
* **Structure:** The files are organized by microservice, representing the final state of the code after the LLM applied the refactoring operations.

### 4. ğŸ·ï¸ `Refactoring_LLM/Labels.md` (Evaluation)
This file contains the ground truth and evaluation metrics.
* **Methodology:** The generated outputs were subjected to a **blind review** by three independent senior developers.
* **Metrics:** The file presents the individual ratings and the final **Consensus Score** (Low/Medium/High) regarding the correctness, readability, and architectural improvement of the refactoring.

---

## ğŸ¯ Case Studies

The experiment was conducted on the following open-source microservice benchmarks:

* **[TeaStore](https://github.com/DescartesResearch/TeaStore):** A reference application for benchmarking and stress testing.
* **[Microservices-observability](https://github.com/aelkz/microservices-observability):** A project demonstrating observability patterns in distributed systems.

## ğŸš€ How to Reproduce

1.  Review **`Prompt.md`** to understand the instructions given to the models.
2.  Examine **`LLM_Input`** to see the initial state of the code and the context provided.
3.  Compare the files in **`LLM_Outputs`** against the original files in `LLM_Input` to observe the specific refactoring changes applied by the LLM.
4.  Consult **`Refactoring_LLM/Labels.md`** to see how human experts evaluated these specific changes.

---

## âœ‰ï¸ Contact & Citation

If you utilize this dataset in your research, please refer to the associated workshop paper. For inquiries regarding the dataset integrity or evaluation criteria, please open an Issue in this repository.
