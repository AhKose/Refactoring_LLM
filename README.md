# Replication Package: Empirical Analysis of LLM-Driven Refactoring for Microservices

This repository serves as the replication package for the study titled **"An Empirical Analysis of LLM-Driven Refactoring for Microservice Systems."**

It contains the experimental datasets, the subject selection criteria based on expert labeling, the 3-phase prompting protocol, and the final refactored code artifacts resulting from the integration of Large Language Model (LLM) suggestions.

## üìå Abstract

Large Language Models (LLMs) are increasingly used in software engineering, raising the question of whether they can reliably support refactoring in microservice‚Äëbased systems. This study empirically examines LLM‚Äëdriven refactoring using a structured, three‚Äëphase prompting protocol applied to **four microservices specifically selected for their low maintainability.**

The selection was based on a prior ground-truth dataset where three senior developers labeled microservices as Low, Medium, or High maintainability. Using this "Low" subset, three models, **GPT‚Äë5.2 Extended Thinking**, **Claude Opus 4.5**, and **Gemini 3 Pro**, were evaluated. Of twelve total refactoring experiments, eleven successfully passed build and run verification. The findings indicate that LLM‚Äëdriven, developer‚Äëassisted refactoring can produce measurable improvements in code complexity and maintainability.

---

## üìÇ Repository Structure

The repository is organized to reflect the **Selection $\rightarrow$ Input $\rightarrow$ Process $\rightarrow$ Output** workflow:

### 1. üè∑Ô∏è `Refactoring_LLM/Labels.md` (Subject Selection / Ground Truth)
This file contains the **pre-study expert labels** used to select the subject systems for this experiment.
* **Context:** In a preliminary study, three senior developers analyzed the microservices and labeled their maintainability levels (Low, Medium, High).
* **Usage:** This dataset served as the **inclusion criteria** for the current study. Only microservices with a consensus label of **"LOW Maintainability"** were selected for the LLM-driven refactoring experiment.

### 2. üìÑ `Prompt.md` (Experimental Protocol)
Documents the **3-Stage Chain-of-Thought Prompting Strategy** utilized in the study.
* **Turn 1:** Context Loading & Architectural Analysis.
* **Turn 2:** Refactoring Implementation (Code Generation).
* **Turn 3:** Self-Audit & Justification.

### 3. üì• `Input_Files/` (Context & Targets)
Contains the source artifacts prepared for the LLM context window.
* **Content:** Automated aggregation of `.java` files from the "Low Maintainability" microservices.
* **Structure:** Includes both target files (for refactoring) and read-only context files (for dependency awareness).

### 4. üì§ `Refactored_Files/` (Refactored Artifacts)
Contains the **final state of the source code** after the manual integration of LLM suggestions.
* **Content:** These are **NOT** raw chat logs. Instead, they are the valid Java source files where the code changes proposed by the LLMs (in Turn 2) have been **manually applied** to the specific files requiring refactoring.
* **Purpose:** These files represent the "Post-Refactoring" state used for metric calculation and testing.

---

## üî¨ Experimental Workflow & Methodology

The study followed a rigorous **Subject Selection $\rightarrow$ Input Generation $\rightarrow$ Integration $\rightarrow$ Validation** workflow.

### 1. Subject Selection (Ground Truth)
Microservices were selected based on the expert consensus found in `Refactoring_LLM/Labels.md`.
* **Criteria:** Services labeled as **"LOW"** (Low Maintainability) by 3 senior developers were isolated as candidates for refactoring.

### 2. Data Preparation
To provide the LLMs with a structured context, an automated Python script aggregated the source code.
* **Constraints:** Context capped at 60,000 characters; files prefixed with `### FILE: <path>` headers to maintain structural awareness.

### 3. Execution & Manual Integration
The refactoring proposals generated by the LLMs were **manually integrated** into the source tree.
* **Process:** The developer took the code snippets generated by the LLM and replaced the corresponding legacy logic in the target files.
* **Result:** The modified files are stored in `LLM_Outputs/`.

### 4. Verification & Validation Framework
Following integration, each refactored microservice underwent a mandatory verification sequence.

| Subject System | Validation Method |
| :--- | :--- |
| **TeaStore** | **1. Build & Deploy:** Executed using the project-supported deployment mechanism.<br>**2. Smoke Tests:** Manual UI tests on localhost to verify core user-facing flows.<br>**3. Regression Testing:** Execution of existing unit tests. |
| **Microservices-<br>observability** | **1. Build:** Maven/Jar-based successful compilation.<br>**2. Health Check:** Verification via localhost health endpoint (OK status).<br>**3. Trace Analysis:** Issued API requests (POST) and confirmed via **Jaeger** that distributed traces were generated without errors. |

---

## üìä Key Findings

The study evaluated **12 service‚Äìmodel refactoring outcomes** (4 Services √ó 3 Models).

* **Reliability:** **11 out of 12** refactored services successfully passed the build and runtime verification pipeline.
* **Metric Improvements:**
    * **Cognitive Complexity (Max):** Reduced by approx. **50%**.
    * **Cyclomatic Complexity (Max):** Reduced by approx. **30%**.
    * **Code Duplication:** Decreased by **7.5%**.
    * **Code Size:** LOC increased by ~6.4% due to the extraction of private helper methods (decomposition strategy).

---

## ‚úâÔ∏è Contact & Citation

For inquiries regarding dataset integrity, please open an Issue in this repository.
