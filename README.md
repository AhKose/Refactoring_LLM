# Replication Package: Empirical Analysis of LLM-Driven Refactoring for Microservices

This repository serves as the replication package for the study titled **"An Empirical Analysis of LLM-Driven Refactoring for Microservice Systems."**

It contains the experimental datasets, the automated input generation scripts, the 3-phase prompting protocol, the raw code artifacts generated by three state-of-the-art Large Language Models (LLMs), and the human evaluation labels used to assess the reliability of AI-supported refactoring.

## üìå Abstract

Large Language Models (LLMs) are increasingly used in software engineering, raising the question of whether they can reliably support refactoring in microservice‚Äëbased systems, where architectural constraints and deployment requirements make refactoring particularly complex. This study empirically examines LLM‚Äëdriven refactoring using a structured, three‚Äëphase prompting protocol applied to four microservices previously labeled as low‚Äëmaintainability.

Three models‚Äî**GPT‚Äë5.2 Extended Thinking**, **Claude Opus 4.5**, and **Gemini 3 Pro**‚Äîwere evaluated. Of twelve total refactorings, eleven successfully passed build and run verification. Overall, the findings indicate that LLM‚Äëdriven, developer‚Äëassisted refactoring can produce measurable and functionally safe improvements in microservice‚Äëbased systems.

---

## üî¨ Experimental Workflow & Methodology

The study followed a rigorouse **Input $\rightarrow$ Process $\rightarrow$ Integration $\rightarrow$ Validation** workflow.

### 1. Data Preparation (Automated Source Extraction)
To provide the LLMs with a structured and manageable context, an **automated Python script** was developed to aggregate the source code from the original repositories.
* **Source Extraction:** All `.java` files from the `src/main/java` directory of the target microservices were extracted.
* **Context Optimization:** To mitigate hallucinations and stay within optimal context windows, aggregated text files were capped at a **60,000-character limit**.
* **File Integrity:** Files were never split across context chunks. Each file was prefixed with a standardized header (`### FILE: <path>`) to maintain directory context for the LLM.
* **Architectural Context:** A read-only "Manifesto" and `pom.xml` files were bundled to provide dependency awareness.

### 2. LLM Inference (3-Phase Prompting)
The refactoring was performed sequentially using a **Fresh Session** strategy for each microservice to prevent cross-session contamination.
* **Protocol:** See [`Prompt.md`](Prompt.md) for the detailed 3-stage chain-of-thought strategy.

### 3. Execution & Manual Integration
The raw code blocks generated by the LLMs (stored in `LLM_Outputs/`) were **manually integrated** into the original project structures.
* **Mapping:** Code was replaced based on the line-number mappings provided by the models.
* **Versioning:** Each refactored version was saved as a distinct project iteration to allow for side-by-side comparison with the legacy code.

### 4. Verification & Validation Framework
Following integration, each refactored microservice underwent a mandatory verification sequence aligned with the specific capabilities of the subject system.

| Subject System | Validation Method |
| :--- | :--- |
| **TeaStore** | **1. Build & Deploy:** Executed using the project-supported deployment mechanism.<br>**2. Smoke Tests:** Manual UI tests on localhost to verify core user-facing flows.<br>**3. Regression Testing:** Execution of existing unit tests to detect logic errors. |
| **Microservices-<br>observability** | **1. Build:** Maven/Jar-based successful compilation.<br>**2. Health Check:** Verification via localhost health endpoint (OK status).<br>**3. Trace Analysis:** Issued API requests (POST) and confirmed via **Jaeger** that distributed traces were generated without errors. |

### 5. Quantitative Metric Extraction
To quantify code-quality changes, a **custom script** was applied to both the original and refactored versions of each microservice. This ensured that observed differences resulted strictly from refactoring rather than manual measurement variability.

---

## üìÇ Repository Structure

### 1. üìÑ `Prompt.md`
The exact prompt templates used for the 3-phase execution.

### 2. üì• `LLM_Input/`
The prepared datasets fed into the LLMs.
* **Target Files:** Source code aggregated by the Python script.
* **Context Files:** Read-only dependencies.

### 3. üì§ `LLM_Outputs/`
The raw output artifacts from the LLMs.
* **Note:** These files represent the specific code changes proposed by the models before integration.

### 4. üè∑Ô∏è `Refactoring_LLM/Labels.md`
The qualitative evaluation data.
* **Consensus:** Blind review scores from three senior developers regarding correctness and quality.

---

## üìä Key Findings

The study evaluated **12 service‚Äìmodel refactoring outcomes** (4 Services √ó 3 Models).

* **Reliability:** **11 out of 12** refactorings successfully passed the build and runtime verification pipeline described above.
* **Metric Improvements:**
    * **Cognitive Complexity (Max):** Reduced by approx. **50%**.
    * **Cyclomatic Complexity (Max):** Reduced by approx. **30%**.
    * **Code Duplication:** Decreased by **7.5%**.
    * **Code Size:** LOC increased by ~6.4% due to the enforced decomposition strategy (extracting private helper methods).

---

## ‚úâÔ∏è Contact & Citation

If you utilize this dataset or replication package in your research, please refer to the associated workshop paper. For inquiries regarding dataset integrity, please open an Issue in this repository.
